---
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Time Series Analysis--Project
<br/>

In this project, you will apply various time series modeling techniques to forecast monthly auto sales in the U.S. using data from January 1995 to December 2020. The data set contains five variables listed below

* Month: a numeric variable that takes integer values from 1 to 12.
* Autosales: a numeric variable that shows the level of auto sale of a month.
* Inflation: a numeric variable that shows the change in price level in percentage.
* PPI: producer price index that measures of the change in prices that domestic producers receive for their goods and services.
* GasPrices: price of gasoline.

The project will consist of the following parts:

1. Exploratory Data Analysis and fit/hold-out sample split
2. Univariate Time-series Models:
    + 2.1 Constant Mean Model
    + 2.2 Random Walk Model
    + 2.3 Random Walk Model with Drift
    + 2.4 Seasonal Dummies and Trend
    + 2.5 Cyclical Trend Model
3. Time Series Regression Models:
    + 3.1 Discussion of independent variables. Correlation analysis and scatter plots
    + 3.2 Regression modeling with external predictors
4. Stochastic Time Series Models:
    + 4.1 Analysis of modeling of deterministic time series model residuals if they are stationary
    + 4.2 Analysis of modeling of regression model residuals
    + 4.3 ARIMA models for variable of interest (with seasonal ARIMA components if applicable)
5. Model comparisons 
<br/><br/>

Libraries
```{r}
# Import all your libraries here
library(TSA)
library(base)
library(car)
library(carData)
library(datasets)
library(ggplot2)
library(graphics)
library(grDevices)
library(methods)
library(skedastic)
library(stats)
library(units)
```

Functions
```{r}
error_eval<-function(prediction,actual,type){
    
    # The function evaluates forecasting error based on predictions and actual values of the series
    # 
    # Inputs:
    # 
    # -predictions (numeric): a vector contains predicted values over time
    # -actual (numeric): the actual values of the series
    # -type (character): "mse" for mean squared error, "mape" for mean absolute percentage error
    #
    # Output:
    #
    # -error (numeric): a number shows the error based on the type specified.
      if(type=="mse"){
    error = mean((actual - prediction)^2)
    print(sprintf("The mean squared error is %.2f", error))
  }
  if(type=="mape"){
    error = mean(abs((actual - prediction) / actual))*100
    print(sprintf("The mean absolute percentage error is %.2f percent", error))
  }

    
    

    
}
```


Part 1: Exploratory Data Analysis and fit/hold-out sample split

1.1 Import raw data and show top 5 rows of the data set
```{r}
# your codes
auto<-read.csv('autosale.csv',header=TRUE)
attach(auto)
head(auto, n = 5)
auto
```
1.2 Create the time series plot of the series Autosales from Jan 1995 to Dec 2020. 
```{r}
# your codes
AS=ts(data=Autosales, start=c(1995, 1), frequency=12)
ts.plot(AS,col="red",lwd="2",ylab="Autosales",main="Autosales in US Jan 1995-Dec 2020")
```

1.3 Create monthly boxplot of the series Autosales.

```{r}
# your codes here
boxplot(Autosales/1000~Month,xlab="Month",ylab="Autosales in (000s)",col="blue")

```



1.4 Create the SACF of Autosales including the first 24 lags

```{r}
# your codes here
acf(Autosales,col="blue",lwd="1", lag=24)
```


1.5 Based on the time series plot, seasonal box plot, and the SACF, do you think the series is stationary? Please justify your answer in terms of trend, seasonality and autocorreltions.

```{r}
# your answer

# I do not think the series is stationary. 
# Firstly, a time series is stationary when it has a constant mean, a constant variance. The time series plot has a decreasing trend, which means that it does not have constant mean.  
# Secondly, the box plot shows that the series does not have similar mean and similar variance. This may mean that the series is not stationary.
# Finally, in accordance with the SACF, all bars are over 2 se bounds.
```

1.6 Split the orginal dataframe into two dataframes: period of fit that contains the first 287 rows, call it "fit", and hold-out sample that contains the rest of the data, call it "hold_out".

```{r}
# your codes
fit<-auto[1:287,]
hold_out<-auto[288:nrow(auto),]
```

Part 2: Univariate Time-series Models
<br/><br/>
2.1 *Constant Mean Model*
<br/><br/>
2.1.1 Perform a Box-perice test with 20 lags for Autosales using data belong to the period of fit.

```{r}
# your codes here
Box.test(fit$Autosales,type=c("Box-Pierce"),lag=20)
```

2.1.2 What is your conclusion of the test? Based on your conclusion, do you think the constant mean model is appropriate for the series?

```{r}
# your answer
# Because p-value < 2.2e-16 is less than 0.05, we could reasonably reject the null hypothesis, at least one ρ not equals to 0, thus the series has significant autcorrelations, the constant mean model is not appropriate for the series.
```

2.1.3 Regardless of the answer to 2.1.2, please fit a constant mean model and plot the predicted mean and 95% confidence interval against the actual series

```{r}
# your codes here
cm <- lm(fit$Autosales~1)
summary(cm)
confint=predict(cm,interval="confidence", level=0.95)
conflower = confint[,2]
confupper = confint[,3]
plot.ts(fit$Autosales/1000, main="Actual versus Predicted Autosales", ylab="Autosales in (000s)",col="blue")
lines(predict(cm)/1000,col="red")
lines(conflower/1000,,col="green")
lines(confupper/1000,,col="green")
```

2.1.4 Please calculate and display the mse and mape associated with the hold-out sample

```{r}
#your codes here
p1=predict(cm, data.frame(hold_out), interval="prediction")
error_eval(p1[,1],hold_out$Autosales,"mse")
error_eval(p1[,1],hold_out$Autosales,"mape")
```

2.2 *Random Walk*
<br/><br/>
2.2.1 Please obtain the first-differenced series and display side-by-side the plot of the differenced series and its SACF. 

```{r}
# your codes here
diff1<-diff(Autosales)
plot.ts(diff1,col="red",lwd=2)
acf(diff1,col="blue")
```

2.2.2 Please perform a Box-pierce test for the differenced series. Do you think a random walk model is appropriate for autosale?

```{r}
# your answer
Box.test(diff1,type=c("Box-Pierce"))
# Based on SACF plot of difference, the original series is different than random walk, because this series is not a White Noise, lags are over 2se bounds. Also, based on the box-pierce test, the p-value 6.402e-11 is less than 0.05, we could reasonably reject the null hypothesis, at least one ρ not equals to 0, thus the series is not White Noise and the original series is different than random walk. So, a random walk model is not appropriate for autosale.
```

2.2.3 Regardless of your answer to 2.2.2, please fit a random walk model using the period of fit and plot the predicted values against the actual series.

```{r}
# your codes here
n=length(fit$Autosales)
PR_Autosales=rep(0,n)
PR_Autosales[1]=NA
res1=rep(0,n-1)

for (t in 2:n){
  PR_Autosales[t]=fit$Autosales[t-1]
  res1[t-1]=fit$Autosales[t]-PR_Autosales[t]
}


ts.plot(fit$Autosales,lwd=2,ylab="Daily Price",main="Random Walk for Autosales")
lines(PR_Autosales,col="red",type="o")
```

2.2.4 Using a Box-Pierce test, can you conclude that the residuals of the random walk model is WN?

```{r}
#your codes here
Box.test(res1,type=c("Box-Pierce"))
# Based on the box-pierce test, the p-value 5.802e-10 is less than 0.05, we could reasonably reject the null hypothesis, at least one ρ not equals to 0. Thus the residuals of the random walk model is not White Noise.
```

2.2.5 Please calculate and display the mse and mape of the random walk model using the hold-out sample

```{r}
# your codes here
h=length(hold_out$Autosales)
PR_Autosalesh=rep(0,h)
PR_Autosalesh[1]=NA

for (t in 2:h){
  PR_Autosalesh[t]=hold_out$Autosales[t-1]
}
error_eval(PR_Autosalesh[2:h],hold_out$Autosales[2:h],"mse")
error_eval(PR_Autosalesh[2:h],hold_out$Autosales[2:h],"mape")
```

2.3 *Random Walk with drift*
<br/><br/>

2.3.1 Suppose we want to fit the series using a random walk model with a drift term, please obtain the estimate of the drift term. 

```{r}
# your codes here
rwd<-lm(diff1~1)
summary(rwd)
C=rwd$coef
C
```

2.3.2 Do you think the drift is necessary?

```{r}
# your answer
# Because the p-value for drift is 0.867, which is much larger than 0.05. Thus, the drift is not necessary.
```

2.3.3 Regardless of your answer to 2.2.2, please fit a random walk model with a drift term using the period of fit and plot the predicted values against the actual series.


```{r}
# your codes here
n=length(fit$Autosales)
PR_Autosalesc=rep(0,n)
PR_Autosalesc[1]=NA
res2=rep(0,n-1)

for (t in 2:n){
  PR_Autosalesc[t]=C+fit$Autosales[t-1]
  res2[t-1]=fit$Autosales[t]-PR_Autosalesc[t]
}

plot.ts(fit$Autosales,ylab="Autosales",main="Actual versus Predicted Values of Autosales")
lines(PR_Autosalesc,col="red", type="b")
```

2.3.4 Please calculate and display the mse and mape of the random walk model using the hold-out sample

```{r}
# your codes here
c=length(hold_out$Autosales)
PR_Autosaleshc=rep(0,c)
PR_Autosaleshc[1]=NA

for (t in 2:c){
  PR_Autosaleshc[t]=C+hold_out$Autosales[t-1]
}
error_eval(PR_Autosaleshc[2:c],hold_out$Autosales[2:c],"mse")
error_eval(PR_Autosaleshc[2:c],hold_out$Autosales[2:c],"mape")
```
2.3.5 Using a Box-Pierce test, can you conclude that the residuals of the random walk model with a drift is WN?

```{r}
#your codes here
Box.test(res1,type=c("Box-Pierce"))
# Based on the box-pierce test, the p-value 5.802e-10 is less than 0.05, we could reasonably reject the null hypothesis, at least one ρ not equals to 0. Thus the residuals of the random walk model is not White Noise.
```

2.4 *Seasonal Dummies and Trend*
<br/><br/>
2.4.1 Create a time series plot of the log-transformed series of Autosale

```{r}
#your codes here

plot.ts(log(Autosales), main="Log-transformed series of Autosale",
ylab="log(Autosales)",col="blue")
```
2.4.2 Estimate a model for log-transformed Autosale with seasonal dummies and a linear trend

```{r}
#your codes here
time<-seq(1, length(fit$Autosales))
sl<-lm(log(fit$Autosales)~time+as.factor(fit$Month))
summary(sl)
```

2.4.3 Estimate a model for log-transformed Autosale with seasonal dummies and a polynomial trend with order 5.
```{r}
#your codes here
k=5
psl<-lm(log(fit$Autosales)~poly(time,k)+as.factor(fit$Month))
summary(psl)
```

2.4.4 Based on adjusted R^2, do you prefer the model from 2.4.2 with a linear trend or the model from 2.4.3 with a polynomial trend?

```{r}
#your codes here
#Based on the adjusted R^2, I prefer the model with a polynomial trend, because the model with a polynomial trend has larger adjusted R^2, and the difference between R^2 and adjusted R^2 for the model with a polynomial trend is smaller.
```


2.4.5 Based on the answer to 2.4.4, plot the predicted series against the actual series for the period of fit.

```{r}
# your codes here
plot.ts(fit$Autosales/1000, main="Actual versus Predicted Autosales", ylab="Autosales in (000s)",col="blue")
lines(exp(predict(psl))/1000,col="red")
```
2.4.6. Obtain the SACF of the residuals. Is the residual WN?
### The residual need exp?

```{r}
# your codes here
res=residuals(psl)
acf(res,main="ACF of Regression Residuals",col="blue")
# The residual is not White Noise, because nearly all bars are over 2 se bounds.

```

2.4.7. Please calculate and display the mse and mape associated with the hold-out sample

```{r}
# your codes here
error_eval(exp(predict(psl,data.frame=c(hold_out$Month,hold_out$Inflation,hold_out$PPI,hold_out$GasPrices))),hold_out$Autosales,"mse")
error_eval(exp(predict(psl,data.frame=c(hold_out$Month,hold_out$Inflation,hold_out$PPI,hold_out$GasPrices))),hold_out$Autosales,"mape")
```

2.5 *Cyclical Trend Model*
<br/><br/>

2.5.1 Obtain the detrend series of Autosale (you are free to choose the order of polynomial for the time trend)

```{r}
#your codes here
k=5
detrend<-lm(fit$Autosales~poly(time,k))

```

2.5.2 Obtain and plot the periodogram of the detrended series.

```{r}
#your codes here
library(TSA)
prdgrm=periodogram(detrend$residuals,col="blue")
```

2.5.3 What are the most important periods based on the periodogram? Are there any "hidden" periods larger than 12?

```{r}
#your answer
period=1/prdgrm$freq

par(mfrow=c(1,2))
periodogram(detrend$residuals,col="blue")
plot(period,prdgrm$spec, type="h",col="blue",ylab="Peridogram",lwd=2)
frequency=prdgrm$freq
amplitude=prdgrm$spec
all=cbind(period,frequency,amplitude)
all <- as.data.frame(all)
per <- subset(head(all[order(all$amplitude, decreasing = TRUE), ]))
per
# important periods: 12.0, 72.0, 96.0, 2.4, 3.0, 4.0
# yes
```

2.5.4 Create the necessary since and cosine paris based on the periods you have identified in 2.5.3 and fit a cyclical model with polynomial trend (the order of the polynomial should be consistent with 2.5.1)

```{r}
# your codes here
n_auto=length(fit$Autosales)
n_auto
times1=1:287
cos1=cos(2*pi*(24/n_auto)*time)
sin1=sin(2*pi*(24/n_auto)*time)

cos2=cos(2*pi*(4/n_auto)*time)
sin2=sin(2*pi*(4/n_auto)*time)

cos3=cos(2*pi*(3/n_auto)*time)
sin3=sin(2*pi*(3/n_auto)*time)

cos4=cos(2*pi*(120/n_auto)*time)
sin4=sin(2*pi*(120/n_auto	)*time)

cos5=cos(2*pi*(96/n_auto)*time)
sin5=sin(2*pi*(96/n_auto	)*time)



pc<-lm(fit$Autosales~poly(time,k)+cos2+sin2+cos3+sin3+cos4+sin4+cos5+sin5
          )
summary(pc)
```

2.5.5 Plot the predicted against the acutal series for the period of fit

```{r}
# your codes here
plot.ts(fit$Autosales, type="b",col="blue",ylab="Autosales",main="Actual versus Predicted Autosales",lwd=2)
lines(predict(pc),col="red",lwd=2)
```


2.5.6 Obtain the SACF of the residuals. Is the residual WN?

```{r}
# your codes here
res1=residuals(pc)
acf(res1,main="ACF of Regression Residuals",col="blue")
# The residual is not White Noise, because some bars are over 2 se bounds.
```

2.5.7 Please calculate and display the mse and mape associated with the hold-out sample

```{r}
# your codes here
error_eval(predict(pc,data.frame=c(hold_out$Month,hold_out$Inflation,hold_out$PPI,hold_out$GasPrices)),hold_out$Autosales,"mse")
error_eval(predict(pc,data.frame=c(hold_out$Month,hold_out$Inflation,hold_out$PPI,hold_out$GasPrices)),hold_out$Autosales,"mape")
```
```
Part 3: Time Series Regression Models
<br/><br/>
3.1 Create scatter plots and correlation estimates of all variables.
```
```{r}
# your answer
pairs(auto,lower.panel = NULL,col="blue")
cor(auto)
```
3.2 Based on 3.1, do you think any of these predictors are informative in predicting autosale?

```{r}
# your answer
#yes,Unemploy,PPI could predict the autosale since the correlations is higher,lower than +-0.3
```

3.3 Estimate the regression model for autosale using all independent variables based on the period of fit.

```{r}
# your codes here
ols1<-lm(fit$Autosales~fit$Month+fit$Inflation+fit$PPI+fit$GasPrices)
summary(ols1)
```

3.4 Plot the fitted values against the actual series for the period of fit.

```{r}
# your codes here
ts.plot(fit$Autosales, ylab="CRESTMS",main="Actual versus predicted values of auto sale" )
lines(predict(ols1), col="red",lwd=2)
```

3.5 Obtain residuals of the model. Are the residuals WN? 

```{r}
# your codes here
res=residuals(ols1)
acf(res,main="ACF of Regression Residuals",col="red",lag=24)
Box.test(res, lag=20)
#no because the p value is <0.05 then we reject the H0
```

3.6 Is the constant variance assumption for the residuals violated based on the white test?

```{r}
# your codes here
library(skedastic)
white(ols1, interactions = TRUE)
# The constant variance assumption is violated, because the p-value 0.005103044 is less than 0.05.
```

3.7 Is there any multicollinearity problem? If yes, what variable(s) would you like to drop from the model

```{r}
# your answer
library(car)
vif(ols1)
#there is no multicollinearity problem, because there is no VIF >> 10.
```

3.8 Please calculate and display the mse and mape associated with the hold-out sample

```{r}
# your codes here
error_eval(predict(ols1,data.frame=c(hold_out$Month,hold_out$Inflation,hold_out$PPI,hold_out$GasPrices)),hold_out$Autosales,"mse")
error_eval(predict(ols1,data.frame=c(hold_out$Month,hold_out$Inflation,hold_out$PPI,hold_out$GasPrices)),hold_out$Autosales,"mape")
```






